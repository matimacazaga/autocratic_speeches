{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import unidecode\n",
    "import os\n",
    "# from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.abspath(os.getcwd()) # initial working directory\n",
    "output_path = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.generalisimofranco.com/Discursos/discursos/00000.HTM\"\n",
    "url_base = \"http://www.generalisimofranco.com/Discursos/discursos/\"\n",
    "\n",
    "source = requests.get(url).text\n",
    "soup = BeautifulSoup(source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of all years in which Franco gave speeches (each year is itself a link)\n",
    "all_years = soup.find_all('a', href = re.compile(\"000\"))\n",
    "all_years = [i for i in all_years if i.string is not None]\n",
    "all_years = [i for i in all_years if \"Discursos\" in i.string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of (year, link) entries for each year\n",
    "year_list = []\n",
    "for one_year in all_years:\n",
    "    \n",
    "    year_string = one_year.string.replace(\".\", \"\")[-4:] # extract year\n",
    "    year = int(year_string) # convert year from string to int\n",
    "    \n",
    "    extension = one_year['href']\n",
    "    year_link = url_base + str(extension)\n",
    "    \n",
    "    year_list.append([year, year_link])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each year, get the links to each of the speeches\n",
    "all_speeches = []\n",
    "for i in year_list:\n",
    "\n",
    "    year = i[0]\n",
    "    yearly_url = i[1]\n",
    "    yearly_source = requests.get(yearly_url).text\n",
    "    yearly_soup = BeautifulSoup(yearly_source, 'html.parser')\n",
    "    all_yearly_speeches = yearly_soup.find_all('a', href = re.compile('.*({}).*'.format(year))) # all 'a' tags containing key word 'year' in the corresponding href\n",
    "    # all_yearly_speeches = [i for i in all_yearly_speeches if i.string is not None]\n",
    "    # all_yearly_speeches = [i for i in all_yearly_speeches if i.string.startswith('(')] # see e.g. 1954: every link included twice, once with actual title, once with date in parentheses as title\n",
    "    \n",
    "    yearly_speeches = [] # list to be filled with links to all speeches of a single year\n",
    "    count = 0\n",
    "    for speech in all_yearly_speeches:\n",
    "        \n",
    "        extension = speech['href']\n",
    "        speech_link = url_base + str(extension)\n",
    "        \n",
    "        if len(yearly_speeches)>0 and speech_link == yearly_speeches[-1][1]: # check for duplicate links\n",
    "            pass\n",
    "        else:\n",
    "            yearly_speeches.append([year, speech_link, count])\n",
    "            count += 1\n",
    "\n",
    "    all_speeches.append(yearly_speeches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten list\n",
    "all_speeches = [speech for yearly_speeches in all_speeches for speech in yearly_speeches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each speech, concatenate all (sub-)headings and paragraphs into a single text string\n",
    "for speech in all_speeches:\n",
    "    speech_url = speech[1]\n",
    "    speech_source = requests.get(speech_url).text\n",
    "    speech_soup = BeautifulSoup(speech_source, 'html.parser')\n",
    "\n",
    "    all_paragraphs = speech_soup.find('blockquote').find_all(['p', 'span'])\n",
    "\n",
    "    text = str()\n",
    "    for paragraph in all_paragraphs:\n",
    "        content = paragraph.text.replace('\\r', '').replace('\\n', '')\n",
    "        text += ' ' + content\n",
    "\n",
    "    speech.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_franco = pd.DataFrame(all_speeches, columns=['Year', 'Link', 'Count', 'Text']) # convert to df\n",
    "df_franco['Name'] = 'Francisco Franco' # add name column\n",
    "df_franco['doc_id'] = df_franco['Year'].map(str) + '_' + df_franco['Name'] + '_' + df_franco['Count'].map(str)  \n",
    "df_franco.drop(['Link', 'Count'], axis = 1) # drop link and count\n",
    "df_franco = df_franco[['Year', 'Name', 'doc_id', 'Text']] # reorder columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match Peronist documents\n",
    "data_path =  '../data/peronist_speeches/txt/' # where Peronist speech text files are saved\n",
    "peronistas = []\n",
    "for filename in os.listdir(os.path.abspath(data_path)):\n",
    "    \n",
    "    year = int(filename[:4])\n",
    "    name = filename.split('_')[1]\n",
    "    count = filename.split('_')[2].split('.')[0]\n",
    "    doc_id = str(year) + '_' + name + '_' + count\n",
    "    with open(data_path + filename, 'r', encoding=\"utf-8\") as handle:\n",
    "        text = handle.read().replace('\\n', '')\n",
    "    \n",
    "    peronistas.append([year, name, doc_id, text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_peron = pd.DataFrame(peronistas, columns=['Year', 'Name', 'doc_id', 'Text']) # convert to df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_franco, df_peron], sort = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save combined df\n",
    "with open(output_path + 'df.pickle', 'wb') as handle:\n",
    "    pickle.dump(df, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "df.to_csv(output_path + 'df.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
