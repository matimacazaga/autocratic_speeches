# Arbeitskorrelation: independent
geeInd <- gee(resp ~ age + smoke,data = ohio, family = "binomial", corstr = "independence")
sumInd <- summary(geeInd)
sumInd$working.correlation
sumInd$scale
pInd <- 2*pnorm(- abs(sumInd$coef[,3])) # p-Werte
pInd < 0.05 # Signifikanz
sumInd$coefficients
sumInd$coefficients[,3]
# Arbeitskorrelation: independent
geeInd <- gee(resp ~ age + smoke,data = ohio, family = "binomial", corstr = "independence")
sumInd <- summary(geeInd)
sumInd$working.correlation
sumInd$scale
pInd <- 2*pnorm(- abs(sumInd$coef[,3])) # p-Werte
pInd < 0.05 # Signifikanz
# Arbeitskorrelation: exchangeable
geeEx <- gee(resp ~ age + smoke, data = ohio, family = "binomial", corstr = "exchangeable")
sumEx <- summary(geeEx)
sumEx$working.correlation
pEx <- 2*pnorm(- abs(sumEx$coef[,3])) # p-Werte
pEx < 0.05 # Signifikanz
# Arbeitskorrelation: AR(1)
geeAR1 <- gee(resp ~ age + smoke, data = ohio, family = "binomial", corstr = "AR-M", Mv = 1)
sumAR1 <- summary(geeAR1)
sumAR1$working.correlation
pAR1 <- 2*pnorm(- abs(sumAR1$coef[,3])) # p-Werte
pAR1 < 0.05 # Signifikanz
# Arbeitskorrelation: unstructured
geeUnstr <- gee(resp ~ age + smoke, data = ohio, family = "binomial", corstr = "unstructured")
sumUnstr <- summary(geeUnstr)
sumUnstr$working.correlation
sumUnstr$scale
pUnstr <- 2*pnorm(- abs(sumUnstr$coef[,3])) # p-Werte
pUnstr < 0.05 # Signifikanz
install.packages(c("mboost", "TH.data"))
library(mboost)
one_se <- function(object, ...) {
UseMethod("one_se")
}
one_se <- function(object, folds = cv(model.weights(object)), ...) {
# cross-validation with cross-validation, depending on folds-type "bootstrap", "kfold", "subsampling"
cvobj <- cvrisk(object, folds = folds, ...)
k <- dim(cvobj)[1]
# optimal stop point with cross-validaton
mstop_obj <- m_iter <- mstop(cvobj) + 1   #  + 1 due to the cv_obj starts at 0
# calculate the corresponding mean error
mean_mstop_obj <- mean(cvobj[, mstop_obj])
# one_se_rule,
oneSE_rule <- mean(cvobj[, mstop_obj]) + (1/sqrt(k))*sd(cvobj[, mstop_obj])
# optimal stop point according to one_SE_rule
oneSE_value <- mean_mstop_obj
while (oneSE_value <= oneSE_rule) {
m_iter <- m_iter - 1
if (m_iter == 0) {
break
}
oneSE_value <- mean(cvobj[, m_iter])
}
class(oneSE_value) <- "one_se"
attr(oneSE_value, "mstop_classic") <- mstop_obj - 1  # correcting -1, see above
attr(oneSE_value, "se_classic") <- mean_mstop_obj
attr(oneSE_value, "mstop_oneSE") <- m_iter
attr(oneSE_value, "se_oneSE") <- mean(cvobj[, m_iter + 1])
attr(oneSE_value, "cvrisk_output") <- cvobj
oneSE_value
}
print.one_se <- function(x, ...) {
cat("\n\t Optimal number of boosting iteration with", attr(attr(x, "cvrisk_output"), "type"), "in
\t - classic method: \t", attr(x, "mstop_classic"), "\tand SE: ", attr(x, "se_classic"),
"\n\t - one SE Rule:  \t", attr(x, "mstop_oneSE"), "\tand SE: ", attr(x, "se_oneSE"), "\n")
return(invisible(x))
}
plot.one_se <- function(x, ...) {
obj <- attr(x, "cvrisk_output")
plot(obj, ...)
mstoposr <- attr(x, "mstop_oneSE")   # mstoposr: mstop one_se_rule
seosr <- attr(x, "se_oneSE")       # seosr: SE one_se_rule
lines(c(mstoposr, mstoposr), c(0, seosr), col = "red", lty = 2)
legend("topright", legend = c("Classic", "One SE Rule"),
col = c("black", "red"), lty = c(2, 2), bty = "n")
}
mstop.one_se <- function(object, ...) {
attr(object, "mstop") <- as.integer(attr(object, "mstop_oneSE"))
return(attr(object, "mstop"))
}
###############################################################################
# # EXAMPLE
#
data("bodyfat", package = "TH.data")
names(bodyfat)
# model structure for mboost
bf_gam <- glmboost(DEXfat ~ ., data = bodyfat,
control = boost_control(center = TRUE, mstop = 100, trace = T))
# plot cv-plot for the two different methods
set.seed(86)
(c <- cvrisk(bf_gam, folds = cv(model.weights(bf_gam), type = "kfold", B = 5)))
set.seed(86)
(oS <- one_se(bf_gam, folds = cv(model.weights(bf_gam), type = "kfold", B = 5)))
# compare results
c; oS
str(c)
str(oS)
plot(c)
plot(oS)
#' Funktion zur Berechnung eines Kerndichteschätzers
#' @param x Vektor von Punkten, an denen der Kerndichteschätzer ausgewertet werden soll
#' @param data Daten
#' @param K Kernfunktion, Argument = Wert, an dem die Kernfunktion ausgewertet werden soll
#' @param h Bandweite
#'
#' @return Kerndichteschätzer ausgewertet an x
kernDichte <- function(x, data, K, h) {
res <- rep(NA, length(x))
for(i in 1:length(x)) # schneller über sapply(x, function(xi, data, K, h){mean(K((xi-data)/h)/h})
{
u = (x[i] - data)/h
res[i] = mean(K(u))/h
}
return(res)
}
#' Epanechnikov - Kernfunktion
#' @param u Vektor von Punkten, an denen die Funktion ausgewertet werden soll
epanK <- function(u) {
y <- ifelse(abs(u) < 1, 3/4 * (1 - u^2), 0)
return(y)
}
### Kerndichteschätzer für Old Faithful Daten
# Daten laden
data(faithful)
# Überblick über die Daten
boxplot(faithful$waiting)
range(faithful$waiting)
# Kerndichteschätzer berechnen...
x <- seq(40, 100, 0.05)
y <- kernDichte(x, faithful$waiting, K = epanK, h = 5)
# ... und zeichnen
plot(x, y, type = "l", ylab = expression(paste(hat(f), "(x)")))
### Bandbreite variieren
# h = 5 (wie vorher)
plot(x, kernDichte(x, faithful$waiting, K = epanK, h = 5), type = "l",  ylab = expression(paste(hat(f), "(x)")))
# h = 2.5
lines(x, kernDichte(x, faithful$waiting, K = epanK, h = 2.5), col = 2)
# h = 10
lines(x, kernDichte(x, faithful$waiting, K = epanK, h = 10), col = 3)
legend("topleft", legend = c(5, 2.5, 10), title = "Bandweite h", col = 1:3, lty= 1)
#' Rechteck - Kernfunktion (-> Histogramm)
#' @param u Vektor von Punkten, an denen die Funktion ausgewertet werden soll
histK <- function(u) {
y <- ifelse(abs(u) < 1, 1/2, 0)
return(y)
}
#' Gauss-Kernfunktion / Normalkern
#' @param u Vektor von Punkten, an denen die Funktion ausgewertet werden soll
gaussK <- function(u) {
y <- 1/sqrt(2*pi) * exp(-0.5 * u^2)
return(y)
}
# Epanechnikov-Kern (wie vorher)
plot(x, kernDichte(x, faithful$waiting, K = epanK, h = 5),
type = "l",  ylab = expression(paste(hat(f), "(x)")))
# Rechteckskern (-> Histogramm)
lines(x, kernDichte(x, faithful$waiting, K = histK, h = 5), col = 2)
# Gauss-Kern
lines(x, kernDichte(x, faithful$waiting, K = gaussK, h = 5), col = 3)
legend("topleft", legend = c("Epanechnikov", "Rechteck", "Gauss"), title = "Kernfunktion", col = 1:3, lty= 1)
# R-Funktion density für Kerndichteschätzung
?density
# eigene Implementierung (wie vorher: Epanechnikov-Kern und Bandweite = 5)
plot(x, kernDichte(x, faithful$waiting, K = epanK, h = 5),
type = "l",  ylab = expression(paste(hat(f), "(x)")))
# density() mit Epanechnikov-Kern und Bandweitenparameter bw = 5
lines(density(x = faithful$waiting, kernel = "epanechnikov", bw = 5,
from = 40, to = 100, n = length(x)), col = 2, lty = 2)
# Kern hier anders skaliert (vgl. Hilfe)!
# Bandweite mit "korrekter" Skalierung
lines(density(x = faithful$waiting, kernel = "epanechnikov", bw = 5/sqrt(5),
from = 40, to = 100, n = length(x)), col = 3, lty = 2)
legend("topleft", legend = c("Eigene Funktion", "Density (bw = 5)", "Density (bw = 5/sqrt(5))"), col = 1:3, lty= c(1,2,2))
# Paket boot laden
library(boot)
# seed setzen
set.seed(1234)
#' Funktion für Berechnung der Kerndichteschätzung (für die Funktion boot)
#' @param x Vektor der Beobachtungen
#' @param i Indexvektor der Bootstrap-Stichprobe
#' @param ... Weitere Parameter, die an density() übergeben werden können
#'
#' @return Kerndichteschätzer für die gegebene Stichprobe
statdens <- function(x, i, ...){
density(x[i],...)$y
}
# Anzahl BS-Replikationen
B <- 1000
# Bootstrapping (nichtparametrisch)
bootRes <- boot(faithful$waiting, statistic = statdens,
bw = dens$bw, #  verwende gleiche Bandweite wie in usprünglicher Schätzung,
n = length(dens$x), from = min(dens$x), to = max(dens$x), # verwene gleiche x-Werte wie in ursprünglicher Schätzung
R = B, sim = "ordinary")
str(bootRes) # Schätzer für jeden x-Wert
### Kerndichteschätzung für Daten
# Daten laden
data(faithful)
# Schätzer berechnen und plotten
dens <- density(faithful$waiting, kernel = "gaussian") # Gaußkern, default Bandweite und x-Werte
plot(dens, lwd = 2, col = "blue",
xlab = "Wartezeit (min)", ylab = "geschätzte Dichte", main = "Kerndichteschätzung für Wartezeiten")
dens
dens$y
plot(dens$y)
# seed setzen
set.seed(1234)
#' Funktion für Berechnung der Kerndichteschätzung (für die Funktion boot)
#' @param x Vektor der Beobachtungen
#' @param i Indexvektor der Bootstrap-Stichprobe
#' @param ... Weitere Parameter, die an density() übergeben werden können
#'
#' @return Kerndichteschätzer für die gegebene Stichprobe
statdens <- function(x, i, ...){
density(x[i],...)$y
}
# Anzahl BS-Replikationen
B <- 1000
# Bootstrapping (nichtparametrisch)
bootRes <- boot(faithful$waiting, statistic = statdens,
bw = dens$bw, #  verwende gleiche Bandweite wie in usprünglicher Schätzung,
n = length(dens$x), from = min(dens$x), to = max(dens$x), # verwene gleiche x-Werte wie in ursprünglicher Schätzung
R = B, sim = "ordinary")
str(bootRes) # Schätzer für jeden x-Wert
str(bootRes$t)
View(bootRes$t0)
# Plot aller Bootstrap-Kerndichteschätzungen (halbtransparent)
matplot(dens$x, t(bootRes$t), type = "l", lty =  1, col = rgb(0,0,0,0.025),
xlab = "Wartezeit (min)", ylab = "geschätzte Dichte", main = "Kerndichteschätzung für Wartezeiten")
lines(dens, lwd = 2, col = "blue") # Schätzung für alle Daten in blau
# Signifikanzniveau
alpha <- 0.05
# Konfidenzband für jeden x-Wert
bootKI <- matrix(0, nrow = 2, ncol = length(dens$x))
for(i in 1:length(dens$x))
bootKI[,i] <- boot.ci(bootRes, conf = (1-alpha), type = "perc", index = i)$perc[c(4,5)]
str(bootKI)
rm(list=ls())
dev.off()
### Kerndichteschätzung für Daten
# Daten laden
data(faithful)
# Schätzer berechnen und plotten
dens <- density(faithful$waiting, kernel = "gaussian") # Gaußkern, default Bandweite und x-Werte
plot(dens, lwd = 2, col = "blue",
xlab = "Wartezeit (min)", ylab = "geschätzte Dichte", main = "Kerndichteschätzung für Wartezeiten")
# Paket boot laden
library(boot)
# seed setzen
set.seed(1234)
#' Funktion für Berechnung der Kerndichteschätzung (für die Funktion boot)
#' @param x Vektor der Beobachtungen
#' @param i Indexvektor der Bootstrap-Stichprobe
#' @param ... Weitere Parameter, die an density() übergeben werden können
#'
#' @return Kerndichteschätzer für die gegebene Stichprobe
statdens <- function(x, i, ...){
density(x[i],...)$y
}
# Anzahl BS-Replikationen
B <- 1000
# Bootstrapping (nichtparametrisch)
bootRes <- boot(faithful$waiting, statistic = statdens,
bw = dens$bw, #  verwende gleiche Bandweite wie in usprünglicher Schätzung,
n = length(dens$x), from = min(dens$x), to = max(dens$x), # verwene gleiche x-Werte wie in ursprünglicher Schätzung
R = B, sim = "ordinary")
str(bootRes) # Schätzer für jeden x-Wert
str(bootRes$t)
# Plot aller Bootstrap-Kerndichteschätzungen (halbtransparent)
matplot(dens$x, t(bootRes$t), type = "l", lty =  1, col = rgb(0,0,0,0.025),
xlab = "Wartezeit (min)", ylab = "geschätzte Dichte", main = "Kerndichteschätzung für Wartezeiten")
lines(dens, lwd = 2, col = "blue") # Schätzung für alle Daten in blau
# Signifikanzniveau
alpha <- 0.05
# Konfidenzband für jeden x-Wert
bootKI <- matrix(0, nrow = 2, ncol = length(dens$x))
for(i in 1:length(dens$x))
bootKI[,i] <- boot.ci(bootRes, conf = (1-alpha), type = "perc", index = i)$perc[c(4,5)]
str(bootKI)
# Plot der KI
matlines(dens$x, t(bootKI), col = "green", lty = 2, lwd = 2)
rug(faithful$waiting+rnorm(length(faithful$waiting), sd=1), lwd = 0.5)
# alle relevanten Komponenten aus density()-Objekt auslesen
fHat <- dens$y
h <- dens$bw
n <- dens$n
# L2-Norm mit density()-Funktion bestimmen
normK <- density(kernel = "gaussian", give.Rkern = TRUE)
normK
# Standardfehler an jeder Stelle x
se <- sqrt(fHat * normK / (n*h))
# Quantil der Standardnormalverteilung
q <- qnorm((1-alpha/2))
# Konfidenzintervall an jedem x-Wert berechnen und plotten
approxKI <- rbind(fHat - q * se, fHat + q * se)
matlines(dens$x, t(approxKI), col="red", lty=2, lwd=3)
# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
"betareg", "ggcorrplot", "grid", "gridExtra", "huge", "knitr", "mvtnorm",
"quanteda", "reshape2", "scales", "stm", "stringi", "tidyverse", "tm"
)
not_installed <- packages_required[!packages_required %in%
installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
lapply(
not_installed,
install.packages,
repos = "http://cran.us.r-project.org",
dependencies = TRUE,
type = itype
)
}
lapply(packages_required, library, character.only = TRUE)
# set working directory (to folder where this code file is saved)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# load data
data <- readRDS("../data/topic_preprocessing/preprocessed_monthly.rds")
data_corpus <- readRDS("../data/topic_preparation/prep_monthly.rds")
# choose covariates (now for topical prevalence AND content) and number of topics
covar <- "Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) +
s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)"
content_var <- "Partei"
outcome <- ""
prevalence <- as.formula(paste(outcome, covar, sep = "~"))
content <- as.formula(paste(outcome, content_var, sep = "~"))
K <- 15
mod_cont <- readRDS("../data/5_2/mod_cont_monthly.rds")
mod_cont$settings$dim$A # number of parties
K*mod_cont$settings$dim$A # total number of beta-vectors in content model
## table of MAP topic proportions per document (for all topics)
topic_props <- stm::make.dt(
mod_cont,
data$meta[c("Name", "Partei","Datum", "Bundesland")]) %>%
cbind(docname = names(data$documents), .)
## top words per topic (for all topics)
n <- 15 # number of top words displayed per topic, per party, and per topic-party interaction
topic_words <- stm::labelTopics(mod_cont, n = n)
## topic to be evaluated
topic_number <- 1
topic_number_long <- paste0("Topic", topic_number)
## number of top documents to be printed in step (2)
docs_number <- 5
## initialize list with empty labels
topic_cont_labels <- list(
Topic1 = NULL,
Topic2 = NULL,
Topic3 = NULL,
Topic4 = NULL,
Topic5 = NULL,
Topic6 = NULL,
Topic7 = NULL,
Topic8 = NULL,
Topic9 = NULL,
Topic10 = NULL,
Topic11 = NULL,
Topic12 = NULL,
Topic13 = NULL,
Topic14 = NULL,
Topic15 = NULL
)
## (1) inspect most frequent words per topic
topic_words # 20 most frequent words
## (2) evaluate most representative documents per topic
data_corpus$docname <- paste0(data_corpus$Twitter_Username, "_", data_corpus$Jahr, "_", data_corpus$Monat)
repr_docs <-  topic_props %>%
arrange(desc(!!as.symbol(topic_number_long))) %>%
.[1:docs_number, c("Name", "docname", "Datum", "Partei", "Bundesland", topic_number_long)] %>%
left_join(data_corpus[,c("Tweets_Dokument", "docname")],
by = "docname")
substr(repr_docs$Tweets_Dokument[1], 0, 256) # view most representative document
topic_number # topic
scales::percent(repr_docs[topic_number_long][1,1], accuracy = 0.01) # proportion
repr_docs$Name[1] # author/MP
repr_docs$Partei[1] # party
repr_docs$Bundesland[1] # state
repr_docs$Datum[1] # date
## (3) assign label
topic_cont_labels[[topic_number]] <- "right/nationalist"
# repeat for all topics
topic_cont_labels <- list(
Topic1 = "Right/Nationalist 1",
Topic2 = "Miscellaneous 1",
Topic3 = "Left/Humanitarian",
Topic4 = "Housing",
Topic5 = "Innovation",
Topic6 = "Green/Energy",
Topic7 = "Miscellaneous 2",
Topic8 = "Corona",
Topic9 = "Foreign Affairs",
Topic10 = "Election",
Topic11 = "Right/Nationalist 2",
Topic12 = "Miscellaneous 3",
Topic13 = "Miscellaneous 4",
Topic14 = "Twitter/Politics",
Topic15 = "Miscellaneous 5"
)
# pick a topic
topic_number <- 8 # topic number
topic_cont_labels[[topic_number]] # topic label
# show difference in vocabulary usage for two selected political parties for the given topic
plot(mod_cont, type = "perspectives", topics = topic_number,
covarlevels = c("Bündnis 90/Die Grünen", "AfD"), text.cex = 0.8,
plabels = c("B'90/Die Grünen", "AfD"))
# Daten laden
data(faithful)
boxplot(faithful$eruptions, main = "Boxplot Eruptions")
boxplot(faithful$waiting, main = "Boxplot Waiting")
# Funktion kde2d aus MASS-package
library(MASS)
?kde2d
# Auswertung der Dichte auf [1,6] x [40,100] an 50 Gridpunkten in jede Richtung
fHat1 <- kde2d(x = faithful$eruptions, y = faithful$waiting, n = 50, lims = c(1,6,40,100))
str(fHat1)
image(fHat1,
main = "Schätzung mit kde2D (package MASS)", xlab = "Eruptions", ylab = "Waiting")
# Beobachtungen hinzufügen
points(faithful, pch = 20, col = rgb(0,0,0, alpha = 0.3))
# Funktion kde aus ks-package
library(ks)
?kde
fHat2 <- kde(x = cbind(faithful$eruptions, faithful$waiting), gridsize = 50, xmin = c(1,40), xmax = c(6,100))
str(fHat2)
image(x = fHat2$eval.points[[1]], y = fHat2$eval.points[[2]], z = fHat2$estimate,
main = "Schätzung mit kde (package ks)", xlab = "Eruptions", ylab = "Waiting")
# Beobachtungen hinzufügen
points(faithful, pch = 20, col = rgb(0,0,0, alpha = 0.3))
# kde2D
fHat3 <- kde2d(x = faithful$eruptions, y = faithful$waiting,
h = c(ucv(faithful$eruptions), ucv(faithful$waiting)), # optimale Bandweite über Kreuzvalidierung
n = 50, lims = c(1,6,40,100))
str(fHat3)
image(fHat3,
main = "Schätzung mit kde2D (package MASS)\nBandweite über Kreuzvalidierung", xlab = "Eruptions", ylab = "Waiting")
# kde
H_CV <- Hlscv(cbind(faithful$eruptions, faithful$waiting))
fHat4 <- kde(x = cbind(faithful$eruptions, faithful$waiting), H = H_CV, gridsize = 50, xmin = c(1,40), xmax = c(6,100))
str(fHat4)
image(x = fHat4$eval.points[[1]], y = fHat4$eval.points[[2]], z = fHat4$estimate,
main = "Schätzung mit kde (package ks)\nBandweite über Kreuzvalidierung", xlab = "Eruptions", ylab = "Waiting")
H_CVdiag <- Hlscv.diag(cbind(faithful$eruptions, faithful$waiting))
fHat4diag <- kde(x = cbind(faithful$eruptions, faithful$waiting), H = H_CVdiag, gridsize = 50, xmin = c(1,40), xmax = c(6,100))
str(fHat4diag)
image(x = fHat4diag$eval.points[[1]], y = fHat4diag$eval.points[[2]], z = fHat4diag$estimate,
main = "Schätzung mit kde (package ks)\nBandweite über Kreuzvalidierung (Diagonal)", xlab = "Eruptions", ylab = "Waiting")
# Daten laden und plotten
climate <- read.table("moberg2005.raw", header = T)
head(climate)
# Daten laden und plotten
climate <- read.table("moberg2005.raw", header = T)
getwd()
# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
"stringi", "tidyverse"
)
not_installed <- packages_required[!packages_required %in%
installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
lapply(
not_installed,
install.packages,
repos = "http://cran.us.r-project.org",
dependencies = TRUE,
type = itype
)
}
lapply(packages_required, library, character.only = TRUE)
# set working directory (to folder where this code file is saved)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# set working directory (to folder where this code file is saved)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
"stringi", "tidyverse"
)
not_installed <- packages_required[!packages_required %in%
installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
lapply(
not_installed,
install.packages,
repos = "http://cran.us.r-project.org",
dependencies = TRUE,
type = itype
)
}
lapply(packages_required, library, character.only = TRUE)
# set working directory (to folder where this code file is saved)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
df <- read_delim('../data/df.csv', delim = ',')
View(df)
# inspect parsing problems
problems(tweepy_df)
# inspect parsing problems
problems(df)
View(df)
typeof(df$Year)
# plot histogram of tweets per year
hist(df %>% pull(Year))
