# Signifikanzniveau
alpha <- 0.05
# Konfidenzband für jeden x-Wert
bootKI <- matrix(0, nrow = 2, ncol = length(dens$x))
for(i in 1:length(dens$x))
bootKI[,i] <- boot.ci(bootRes, conf = (1-alpha), type = "perc", index = i)$perc[c(4,5)]
str(bootKI)
rm(list=ls())
dev.off()
### Kerndichteschätzung für Daten
# Daten laden
data(faithful)
# Schätzer berechnen und plotten
dens <- density(faithful$waiting, kernel = "gaussian") # Gaußkern, default Bandweite und x-Werte
plot(dens, lwd = 2, col = "blue",
xlab = "Wartezeit (min)", ylab = "geschätzte Dichte", main = "Kerndichteschätzung für Wartezeiten")
# Paket boot laden
library(boot)
# seed setzen
set.seed(1234)
#' Funktion für Berechnung der Kerndichteschätzung (für die Funktion boot)
#' @param x Vektor der Beobachtungen
#' @param i Indexvektor der Bootstrap-Stichprobe
#' @param ... Weitere Parameter, die an density() übergeben werden können
#'
#' @return Kerndichteschätzer für die gegebene Stichprobe
statdens <- function(x, i, ...){
density(x[i],...)$y
}
# Anzahl BS-Replikationen
B <- 1000
# Bootstrapping (nichtparametrisch)
bootRes <- boot(faithful$waiting, statistic = statdens,
bw = dens$bw, #  verwende gleiche Bandweite wie in usprünglicher Schätzung,
n = length(dens$x), from = min(dens$x), to = max(dens$x), # verwene gleiche x-Werte wie in ursprünglicher Schätzung
R = B, sim = "ordinary")
str(bootRes) # Schätzer für jeden x-Wert
str(bootRes$t)
# Plot aller Bootstrap-Kerndichteschätzungen (halbtransparent)
matplot(dens$x, t(bootRes$t), type = "l", lty =  1, col = rgb(0,0,0,0.025),
xlab = "Wartezeit (min)", ylab = "geschätzte Dichte", main = "Kerndichteschätzung für Wartezeiten")
lines(dens, lwd = 2, col = "blue") # Schätzung für alle Daten in blau
# Signifikanzniveau
alpha <- 0.05
# Konfidenzband für jeden x-Wert
bootKI <- matrix(0, nrow = 2, ncol = length(dens$x))
for(i in 1:length(dens$x))
bootKI[,i] <- boot.ci(bootRes, conf = (1-alpha), type = "perc", index = i)$perc[c(4,5)]
str(bootKI)
# Plot der KI
matlines(dens$x, t(bootKI), col = "green", lty = 2, lwd = 2)
rug(faithful$waiting+rnorm(length(faithful$waiting), sd=1), lwd = 0.5)
# alle relevanten Komponenten aus density()-Objekt auslesen
fHat <- dens$y
h <- dens$bw
n <- dens$n
# L2-Norm mit density()-Funktion bestimmen
normK <- density(kernel = "gaussian", give.Rkern = TRUE)
normK
# Standardfehler an jeder Stelle x
se <- sqrt(fHat * normK / (n*h))
# Quantil der Standardnormalverteilung
q <- qnorm((1-alpha/2))
# Konfidenzintervall an jedem x-Wert berechnen und plotten
approxKI <- rbind(fHat - q * se, fHat + q * se)
matlines(dens$x, t(approxKI), col="red", lty=2, lwd=3)
# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
"betareg", "ggcorrplot", "grid", "gridExtra", "huge", "knitr", "mvtnorm",
"quanteda", "reshape2", "scales", "stm", "stringi", "tidyverse", "tm"
)
not_installed <- packages_required[!packages_required %in%
installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
lapply(
not_installed,
install.packages,
repos = "http://cran.us.r-project.org",
dependencies = TRUE,
type = itype
)
}
lapply(packages_required, library, character.only = TRUE)
# set working directory (to folder where this code file is saved)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# load data
data <- readRDS("../data/topic_preprocessing/preprocessed_monthly.rds")
data_corpus <- readRDS("../data/topic_preparation/prep_monthly.rds")
# choose covariates (now for topical prevalence AND content) and number of topics
covar <- "Partei+ Bundesland + s(t, df = 5) + s(Struktur_4, df = 5) +
s(Struktur_22, df = 5) + s(Struktur_42, df = 5) + s(Struktur_54, df = 5)"
content_var <- "Partei"
outcome <- ""
prevalence <- as.formula(paste(outcome, covar, sep = "~"))
content <- as.formula(paste(outcome, content_var, sep = "~"))
K <- 15
mod_cont <- readRDS("../data/5_2/mod_cont_monthly.rds")
mod_cont$settings$dim$A # number of parties
K*mod_cont$settings$dim$A # total number of beta-vectors in content model
## table of MAP topic proportions per document (for all topics)
topic_props <- stm::make.dt(
mod_cont,
data$meta[c("Name", "Partei","Datum", "Bundesland")]) %>%
cbind(docname = names(data$documents), .)
## top words per topic (for all topics)
n <- 15 # number of top words displayed per topic, per party, and per topic-party interaction
topic_words <- stm::labelTopics(mod_cont, n = n)
## topic to be evaluated
topic_number <- 1
topic_number_long <- paste0("Topic", topic_number)
## number of top documents to be printed in step (2)
docs_number <- 5
## initialize list with empty labels
topic_cont_labels <- list(
Topic1 = NULL,
Topic2 = NULL,
Topic3 = NULL,
Topic4 = NULL,
Topic5 = NULL,
Topic6 = NULL,
Topic7 = NULL,
Topic8 = NULL,
Topic9 = NULL,
Topic10 = NULL,
Topic11 = NULL,
Topic12 = NULL,
Topic13 = NULL,
Topic14 = NULL,
Topic15 = NULL
)
## (1) inspect most frequent words per topic
topic_words # 20 most frequent words
## (2) evaluate most representative documents per topic
data_corpus$docname <- paste0(data_corpus$Twitter_Username, "_", data_corpus$Jahr, "_", data_corpus$Monat)
repr_docs <-  topic_props %>%
arrange(desc(!!as.symbol(topic_number_long))) %>%
.[1:docs_number, c("Name", "docname", "Datum", "Partei", "Bundesland", topic_number_long)] %>%
left_join(data_corpus[,c("Tweets_Dokument", "docname")],
by = "docname")
substr(repr_docs$Tweets_Dokument[1], 0, 256) # view most representative document
topic_number # topic
scales::percent(repr_docs[topic_number_long][1,1], accuracy = 0.01) # proportion
repr_docs$Name[1] # author/MP
repr_docs$Partei[1] # party
repr_docs$Bundesland[1] # state
repr_docs$Datum[1] # date
## (3) assign label
topic_cont_labels[[topic_number]] <- "right/nationalist"
# repeat for all topics
topic_cont_labels <- list(
Topic1 = "Right/Nationalist 1",
Topic2 = "Miscellaneous 1",
Topic3 = "Left/Humanitarian",
Topic4 = "Housing",
Topic5 = "Innovation",
Topic6 = "Green/Energy",
Topic7 = "Miscellaneous 2",
Topic8 = "Corona",
Topic9 = "Foreign Affairs",
Topic10 = "Election",
Topic11 = "Right/Nationalist 2",
Topic12 = "Miscellaneous 3",
Topic13 = "Miscellaneous 4",
Topic14 = "Twitter/Politics",
Topic15 = "Miscellaneous 5"
)
# pick a topic
topic_number <- 8 # topic number
topic_cont_labels[[topic_number]] # topic label
# show difference in vocabulary usage for two selected political parties for the given topic
plot(mod_cont, type = "perspectives", topics = topic_number,
covarlevels = c("Bündnis 90/Die Grünen", "AfD"), text.cex = 0.8,
plabels = c("B'90/Die Grünen", "AfD"))
# Daten laden
data(faithful)
boxplot(faithful$eruptions, main = "Boxplot Eruptions")
boxplot(faithful$waiting, main = "Boxplot Waiting")
# Funktion kde2d aus MASS-package
library(MASS)
?kde2d
# Auswertung der Dichte auf [1,6] x [40,100] an 50 Gridpunkten in jede Richtung
fHat1 <- kde2d(x = faithful$eruptions, y = faithful$waiting, n = 50, lims = c(1,6,40,100))
str(fHat1)
image(fHat1,
main = "Schätzung mit kde2D (package MASS)", xlab = "Eruptions", ylab = "Waiting")
# Beobachtungen hinzufügen
points(faithful, pch = 20, col = rgb(0,0,0, alpha = 0.3))
# Funktion kde aus ks-package
library(ks)
?kde
fHat2 <- kde(x = cbind(faithful$eruptions, faithful$waiting), gridsize = 50, xmin = c(1,40), xmax = c(6,100))
str(fHat2)
image(x = fHat2$eval.points[[1]], y = fHat2$eval.points[[2]], z = fHat2$estimate,
main = "Schätzung mit kde (package ks)", xlab = "Eruptions", ylab = "Waiting")
# Beobachtungen hinzufügen
points(faithful, pch = 20, col = rgb(0,0,0, alpha = 0.3))
# kde2D
fHat3 <- kde2d(x = faithful$eruptions, y = faithful$waiting,
h = c(ucv(faithful$eruptions), ucv(faithful$waiting)), # optimale Bandweite über Kreuzvalidierung
n = 50, lims = c(1,6,40,100))
str(fHat3)
image(fHat3,
main = "Schätzung mit kde2D (package MASS)\nBandweite über Kreuzvalidierung", xlab = "Eruptions", ylab = "Waiting")
# kde
H_CV <- Hlscv(cbind(faithful$eruptions, faithful$waiting))
fHat4 <- kde(x = cbind(faithful$eruptions, faithful$waiting), H = H_CV, gridsize = 50, xmin = c(1,40), xmax = c(6,100))
str(fHat4)
image(x = fHat4$eval.points[[1]], y = fHat4$eval.points[[2]], z = fHat4$estimate,
main = "Schätzung mit kde (package ks)\nBandweite über Kreuzvalidierung", xlab = "Eruptions", ylab = "Waiting")
H_CVdiag <- Hlscv.diag(cbind(faithful$eruptions, faithful$waiting))
fHat4diag <- kde(x = cbind(faithful$eruptions, faithful$waiting), H = H_CVdiag, gridsize = 50, xmin = c(1,40), xmax = c(6,100))
str(fHat4diag)
image(x = fHat4diag$eval.points[[1]], y = fHat4diag$eval.points[[2]], z = fHat4diag$estimate,
main = "Schätzung mit kde (package ks)\nBandweite über Kreuzvalidierung (Diagonal)", xlab = "Eruptions", ylab = "Waiting")
# Daten laden und plotten
climate <- read.table("moberg2005.raw", header = T)
head(climate)
# Daten laden und plotten
climate <- read.table("moberg2005.raw", header = T)
getwd()
# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
"stringi", "tidyverse"
)
not_installed <- packages_required[!packages_required %in%
installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
lapply(
not_installed,
install.packages,
repos = "http://cran.us.r-project.org",
dependencies = TRUE,
type = itype
)
}
lapply(packages_required, library, character.only = TRUE)
# set working directory (to folder where this code file is saved)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# set working directory (to folder where this code file is saved)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
"stringi", "tidyverse"
)
not_installed <- packages_required[!packages_required %in%
installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
lapply(
not_installed,
install.packages,
repos = "http://cran.us.r-project.org",
dependencies = TRUE,
type = itype
)
}
lapply(packages_required, library, character.only = TRUE)
# set working directory (to folder where this code file is saved)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
df <- read_delim('../data/df.csv', delim = ',')
View(df)
# inspect parsing problems
problems(tweepy_df)
# inspect parsing problems
problems(df)
View(df)
typeof(df$Year)
# plot histogram of tweets per year
hist(df %>% pull(Year))
# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
"stringi", "tidyverse"
)
not_installed <- packages_required[!packages_required %in%
installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
lapply(
not_installed,
install.packages,
repos = "http://cran.us.r-project.org",
dependencies = TRUE,
type = itype
)
}
lapply(packages_required, library, character.only = TRUE)
# set working directory (to folder where this code file is saved)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getwd()
# load data
df <- read_delim('../data/df_unidecode.csv', delim = ',')
View(df)
# load data
df <- read_delim('../data/df_unidecode.csv', delim = ',')
# inspect parsing problems
problems(df)
View(df)
# plot histogram of tweets per year
hist(df %>% pull(Year))
View(df)
# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
"quanteda", "stringi", "tidyverse"
)
not_installed <- packages_required[!packages_required %in%
installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
lapply(
not_installed,
install.packages,
repos = "http://cran.us.r-project.org",
dependencies = TRUE,
type = itype
)
}
lapply(packages_required, library, character.only = TRUE)
# set working directory (to folder where this code file is saved)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# load data
df <- read_delim('../data/df_unidecode.csv', delim = ',')
View(df)
View(df)
# drop index columns
df <- df %>% dplyr::select(-c(Year, Name, doc_id, Text))
View(df)
# load data
df <- read_delim('../data/df_unidecode.csv', delim = ',')
# inspect parsing problems
problems(df)
View(df)
# drop index columns
df <- df %>% dplyr::select(-c(X1, Unnamed: 0))
df
# drop index columns
df <- df %>% dplyr::select(c(Year, Name, doc_id, Text))
df
class(df$Year)
View(df)
df$Year.unique()
df$Year %>% unique()
df$Year %>% unique() %>% min()
test <- df$Year%%1931
test
# first year
first_year <- df$Year %>% unique() %>% min()
# time index t
df["t"] <- df$Year%%(first_year - 1)
df
corp <- quanteda::corpus(x = df, text_field = "Text")
stopwords_es <- read_lines(
"https://raw.githubusercontent.com/stopwords-iso/stopwords-es/master/stopwords-es.txt"
)
stopwords_es
length(stopwords_es) # 620 stopwords
length(stopwords("es")) # 231 stopwords
# combine all stopwords (amp from '&' and innen from -innen)
stopwords_es_customized <- Reduce(union, list(stopwords_es, stopwords("es")))
length(stopwords_es_customized)
# special characters contained in stopwords
unwanted_array = list(    'Š'='S', 'š'='s', 'Ž'='Z', 'ž'='z', 'À'='A', 'Á'='A', 'Â'='A', 'Ã'='A', 'Ä'='A', 'Å'='A', 'Æ'='A', 'Ç'='C', 'È'='E', 'É'='E',
'Ê'='E', 'Ë'='E', 'Ì'='I', 'Í'='I', 'Î'='I', 'Ï'='I', 'Ñ'='N', 'Ò'='O', 'Ó'='O', 'Ô'='O', 'Õ'='O', 'Ö'='O', 'Ø'='O', 'Ù'='U',
'Ú'='U', 'Û'='U', 'Ü'='U', 'Ý'='Y', 'Þ'='B', 'ß'='Ss', 'à'='a', 'á'='a', 'â'='a', 'ã'='a', 'ä'='a', 'å'='a', 'æ'='a', 'ç'='c',
'è'='e', 'é'='e', 'ê'='e', 'ë'='e', 'ì'='i', 'í'='i', 'î'='i', 'ï'='i', 'ð'='o', 'ñ'='n', 'ò'='o', 'ó'='o', 'ô'='o', 'õ'='o',
'ö'='o', 'ø'='o', 'ù'='u', 'ú'='u', 'û'='u', 'ý'='y', 'ý'='y', 'þ'='b', 'ÿ'='y' )
# convert special characters for stopwords (for text data: done in Python already)
stopwords_es_customized <- stringi::stri_replace_all_fixed(
stopwords_es_customized,
c("ñ", "ü", "á", "é", "í", "ó", "ú"),
c("n", "u", "a", "e", "i", "o", "u"),
vectorize_all = FALSE
)
dfmatrix <- quanteda::dfm(
corp,
remove = stopwords_es_customized
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_url = TRUE,
tolower = TRUE,
verbose = FALSE
)
# build document-feature matrix
dfmatrix <- quanteda::dfm(
corp,
remove = stopwords_es_customized,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_url = TRUE,
tolower = TRUE,
verbose = FALSE
)
# check most frequent words
quanteda::topfeatures(dfmatrix, 20)
# perform word stemming and remove infrequent words
dfmatrix_cleaned <- dfmatrix_cleaned %>%
quanteda::dfm_wordstem(language = "spanish") %>% # word stemming
quanteda::dfm_remove(min_nchar = 4) %>% # removing words with <4 characters
quanteda::dfm_trim(min_termfreq = 5, min_docfreq = 3) # removing words occuring in <3 documents or <5 times overall
getwd()
# perform word stemming and remove infrequent words
dfmatrix <- dfmatrix %>%
quanteda::dfm_wordstem(language = "spanish") %>% # word stemming
quanteda::dfm_remove(min_nchar = 4) %>% # removing words with <4 characters
quanteda::dfm_trim(min_termfreq = 5, min_docfreq = 3) # removing words occuring in <3 documents or <5 times overall
# check most frequent words again
quanteda::topfeatures(dfmatrix_cleaned, 20)
# check most frequent words again
quanteda::topfeatures(dfmatrix, 20)
# convert to stm object (this reduces memory use when fitting stm; see ?stm)
df_preprocessed <- quanteda::convert(dfmatrix, to = "stm")
# save
saveRDS(df_preprocessed, "../data/df_preprocessed")
# Install and load required packages
os <- Sys.info()[["sysname"]] # Get operating system information
itype <- ifelse(os == "Linux", "source", "binary") # Set corresponding installation type
packages_required <- c(
"quanteda", "stringi", "tidyverse"
)
not_installed <- packages_required[!packages_required %in%
installed.packages()[, "Package"]]
if (length(not_installed) > 0) {
lapply(
not_installed,
install.packages,
repos = "http://cran.us.r-project.org",
dependencies = TRUE,
type = itype
)
}
lapply(packages_required, library, character.only = TRUE)
# set working directory (to folder where this code file is saved)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# load data
df <- read_delim('../data/df_unidecode.csv', delim = ',')
# inspect parsing problems
problems(df)
# plot histogram of tweets per year
hist(df %>% pull(Year))
# drop index columns / keep only relevant columns
df <- df %>% dplyr::select(c(Year, Name, doc_id, Text))
# generate time index t
## first year
first_year <- df$Year %>% unique() %>% min()
## time index t
df["t"] <- df$Year%%(first_year - 1)
# build corpus, which by default organizes documents into types, tokens, and sentences
corp <- quanteda::corpus(x = df, text_field = "Text")
# start with defining stopwords
stopwords_es <- read_lines(
"https://raw.githubusercontent.com/stopwords-iso/stopwords-es/master/stopwords-es.txt"
)
length(stopwords_es) # 620 stopwords
length(stopwords("es")) # 231 stopwords contained in quanteda package
# combine all stopwords (amp from '&' and innen from -innen)
stopwords_es_customized <- Reduce(union, list(stopwords_es, stopwords("es")))
# convert special characters for stopwords (for text data: done in Python already)
stopwords_es_customized <- stringi::stri_replace_all_fixed(
stopwords_es_customized,
c("ñ", "ü", "á", "é", "í", "ó", "ú"),
c("n", "u", "a", "e", "i", "o", "u"),
vectorize_all = FALSE
)
# build document-feature matrix
dfmatrix <- quanteda::dfm(
corp,
remove = stopwords_es_customized,
remove_symbols = TRUE,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_url = TRUE,
tolower = TRUE,
verbose = FALSE
)
# check most frequent words
quanteda::topfeatures(dfmatrix, 20)
# manually remove specific tokens
dfmatrix <- dfmatrix %>%
quanteda::dfm_remove(pattern = "#", valuetype = "regex") %>%  # hashtags
quanteda::dfm_remove(pattern = "@", valuetype = "regex")  %>%   # @username
quanteda::dfm_remove(pattern = "(^[0-9]+[,.]?[0-9]+)\\w{1,3}$",  # 10er, 14.00uhr etc.
valuetype = "regex") %>%
quanteda::dfm_remove(pattern = "^[^a-zA-Z0-9]*$",  # non-alphanumerical
valuetype = "regex") %>%
quanteda::dfm_remove(pattern = "^.*(aaa|aeae|fff|hhh|uuu|www).*$",  # interjections (aaaawww etc.)
valuetype = "regex") %>%
quanteda::dfm_remove(pattern = "^(polit|bundesregier|bundestag|deutsch|land|jaehrig|http)", # specific words
valuetype = "regex")
# perform word stemming and remove infrequent words
dfmatrix <- dfmatrix %>%
quanteda::dfm_wordstem(language = "spanish") %>% # word stemming
quanteda::dfm_remove(min_nchar = 4) %>% # removing words with <4 characters
quanteda::dfm_trim(min_termfreq = 5, min_docfreq = 3) # removing words occuring in <3 documents or <5 times overall
# check most frequent words again
quanteda::topfeatures(dfmatrix, 20)
# convert to stm object (this reduces memory use when fitting stm; see ?stm)
df_preprocessed <- quanteda::convert(dfmatrix, to = "stm")
# save
saveRDS(df_preprocessed, "../data/df_preprocessed")
# save
saveRDS(df_preprocessed, "../data/df_preprocessed.rds")
